---
title: LLM Chat Endpoint
description: Implement a streaming chat endpoint that Crayon's frontend can connect to.
---

Your backend needs a single endpoint that receives messages and streams back a response from the LLM.

{/* TODO: Add implementation examples */}
{/* Will cover: */}
{/* - Request format: { threadId, messages } */}
{/* - Streaming response format (SSE) */}
{/* - Example with OpenAI SDK (Node.js) */}
{/* - Example with Python (FastAPI + OpenAI) */}
{/* - How to pass the GenUI schema to the LLM for structured output */}
