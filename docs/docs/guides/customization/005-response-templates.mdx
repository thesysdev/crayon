---
description: Creating and using response templates in Crayon
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Creating Response Templates

Response templates are a powerful feature in Crayon that allow you to control how AI responses are structured and displayed. They combine a React component for visualization with a schema that tells the LLM exactly how to format its response. This guide will show you how to create custom response templates using a recipe card example.

For Crayon, a response template is a React component. Something as simple as this:
```tsx
const responseTemplate = {
    name: "recipe",  // Unique identifier for this template
    Component: RecipeTemplate  // React component that renders the response
}
```

The frontend can send a list of such templates, and the backend can tell the LLM which template to choose for which situation. Crayon automatically handles the rest.
For example, if you wanted to create an agent that helps the users with cooking recipes, you could do so in 3 simple steps:

## Step 1: Creating a `RecipeTemplate` component

For the purpose of this guide, let's create a simple card that renders a cooking recipe. You can either manually create a card or design a simple card on the
<ins>[Thesys Canvas](https://app.thesys.dev/home?redirect_uri=/)</ins> and use the CodeGen feature to generate the React code.

<Tabs groupId="frontend-lang">
<TabItem value="ts" label="TypeScript">
```tsx
import { Card, CardHeader, Tag, TagBlock } from "@crayonai/react-ui";
import { FlaskConical as FlaskConicalIcon } from "lucide-react";
import { StepsItem, Steps } from "@crayonai/react-ui/Steps";

interface RecipeTemplateProps {
  title: string;      // Name of the recipe
  ingredients: string[];  // List of required ingredients
  steps: string[];    // Step-by-step cooking instructions
}

export default function RecipeTemplate(props: RecipeTemplateProps) {
  return (
    <Card variant="card" width="standard">
      <CardHeader
        title={<p style={{ textAlign: "left" }}>{props.title}</p>}
        icon={<FlaskConicalIcon size={"1em"} />}
      />
      <TagBlock>
        {props.ingredients.map((ingredient) => (
          <Tag text={<p style={{ textAlign: "left" }}>{ingredient}</p>} />
        ))}
      </TagBlock>
      <Steps>
        {props.steps.map((step) => (
          <StepsItem key={step}><p>{step}</p></StepsItem>
        ))}
      </Steps>
    </Card>
  );
}
```

</TabItem>
<TabItem value="js" label="JavaScript">
```jsx
import { Card, CardHeader, Tag, TagBlock } from "@crayonai/react-ui";
import { FlaskConical as FlaskConicalIcon } from "lucide-react";
import { StepsItem, Steps } from "@crayonai/react-ui/Steps";

export default function RecipeTemplate(props) {
  return (
    <Card variant="card" width="standard">
      <CardHeader
        title={<p style={{ textAlign: "left" }}>{props.title}</p>}
        icon={<FlaskConicalIcon size={"1em"} />}
      />
      <TagBlock>
        {props.ingredients.map((ingredient) => (
          <Tag text={<p style={{ textAlign: "left" }}>{ingredient}</p>} />
        ))}
      </TagBlock>
      <Steps>
        {props.steps.map((step) => (
          <StepsItem key={step}><p>{step}</p></StepsItem>
        ))}
      </Steps>
    </Card>
  );
}
```
</TabItem>
</Tabs>


## Step 2: Creating a schema for the response template

We need to tell the LLM about the structure in which it needs to respond in order to be compatible with our response template component. For this, we define a zod schema:

```ts
import { z } from "zod";

export const RecipeTemplateSchema = z.object({
  title: z.string(),
  ingredients: z.array(z.string()),
  steps: z
    .array(z.string())
    .describe(
      "The instructions to cook the recipe. Do not prefix it with numbers or use markdown, just the instructions."
    ),
});
```

## Step 3: Creating a backend endpoint to tell the LLM about the schema

Now that we have our template and its schema ready, we need a backend endpoint that our frontend will call to get the agent's response. This endpoint will be responsible for telling the LLM about the template. Crayon provides some handy utility functions through `crayon/stream` to help with this.

<Tabs groupId="frontend-lang">
<TabItem value='ts' label="TypeScript">
```tsx
import { NextRequest } from "next/server";
import OpenAI from "openai";
import {
  fromOpenAICompletion,
  toOpenAIMessages,
  templatesToResponseFormat,
} from "@crayonai/stream";
import { Message } from "@crayonai/react-core";
import { RecipeTemplateSchema } from "@/types/recipe";

export async function POST(req: NextRequest) {
  const { messages } = (await req.json()) as { messages: Message[] };
  const client = new OpenAI();
  const responseFormat = templatesToResponseFormat({
    schema: RecipeTemplateSchema,
    name: "recipe",
    description: "Use this template to generate a recipe",
  });
  const llmStream = await client.chat.completions.create({
    model: "gpt-4o",
    messages: toOpenAIMessages(
      messages
    ) as OpenAI.Chat.ChatCompletionMessageParam[],
    stream: true,
    response_format: responseFormat,
  });
  const responseStream = fromOpenAICompletion(llmStream);
  return new Response(responseStream as unknown as ReadableStream<Uint8Array>, {
    headers: {
      "Content-Type": "text/event-stream",
      "Cache-Control": "no-cache, no-transform",
      Connection: "keep-alive",
    },
  });
}
```
</TabItem>

<TabItem value='js' label="JavaScript">
```jsx
import { NextRequest } from "next/server";
import OpenAI from "openai";
import {
  fromOpenAICompletion,
  toOpenAIMessages,
  templatesToResponseFormat,
} from "@crayonai/stream";
import { Message } from "@crayonai/react-core";
import { RecipeTemplateSchema } from "@/types/recipe";

export async function POST(req) {
  const { messages } = await req.json();
  const client = new OpenAI();
  const responseFormat = templatesToResponseFormat({
    schema: RecipeTemplateSchema,
    name: "recipe",
    description: "Use this template to generate a recipe",
  });
  const llmStream = await client.chat.completions.create({
    model: "gpt-4o",
    messages: toOpenAIMessages(messages),
    stream: true,
    response_format: responseFormat,
  });
  const responseStream = fromOpenAICompletion(llmStream);
  return new Response(responseStream, {
    headers: {
      "Content-Type": "text/event-stream",
      "Cache-Control": "no-cache, no-transform",
      Connection: "keep-alive",
    },
  });
}
```
</TabItem>
</Tabs>

This API can then be used in the `processMessage` method as shown in <ins>[Getting started with Crayon](../001-getting-started.mdx)</ins>, and this response template can be passed to `CrayonChat` as follows:
```tsx
<CrayonChat
  processMessage={processMessage}
  responseTemplates={[
    {
      name: "text",
      Component: TextTemplate,
    },
  ]}
/>
```
