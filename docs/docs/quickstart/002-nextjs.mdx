import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# NextJS

## Create a new NextJS project
<Tabs groupId="js-package-manager">
  <TabItem value="npm" label="npm">
  ```bash
  npx create-next-app@latest my-agent
  ```
  </TabItem>
  <TabItem value="pnpm" label="pnpm">

  ```bash
  pnpm dlx create-next-app@latest my-agent
  ```
  </TabItem>

  <TabItem value="yarn" label="yarn">
  ```bash
  yarn create-next-app@latest my-agent
  ```
  </TabItem>
</Tabs>

## Install the crayon packages:

<Tabs groupId="js-package-manager">
  <TabItem value="npm" label="npm">
```bash
npm install @crayonai/react-ui @crayonai/react-core @crayonai/stream
```
  </TabItem>
  <TabItem value="pnpm" label="pnpm">
```bash
pnpm add @crayonai/crayon-ui @crayonai/react-core @crayonai/stream
```
  </TabItem>
  <TabItem value="yarn" label="yarn">
```bash
yarn add @crayonai/crayon-ui @crayonai/react-core @crayonai/stream
```
  </TabItem>
</Tabs>


## Add the CrayonChat component to your app

<Tabs groupId="frontend-lang">
  <TabItem value="ts" label="TypeScript">
```typescript title="page.tsx"
"use client";

import type { Message } from "@crayonai/react-core";
import { CrayonChat } from "@crayonai/react-ui";
import "@crayonai/react-ui/styles/index.css";

const processMessage = async ({
  threadId,
  messages,
  abortController,
}: {
  threadId: string;
  messages: Message[];
  abortController: AbortController;
}) => {
  const response = await fetch("/api/chat", {
    method: "POST",
    body: JSON.stringify({ threadId, messages }),
    headers: {
      "Content-Type": "application/json",
      Accept: "text/event-stream",
    },
    signal: abortController.signal,
  });
  return response;
};

export default function Home() {
  return <CrayonChat processMessage={processMessage} />;
}
```
  </TabItem>
  <TabItem value="js" label="JavaScript">
```javascript title="page.jsx"
"use client";

import { CrayonChat } from "@crayonai/react-ui";
import "@crayonai/react-ui/styles/index.css";

const processMessage = async ({ threadId, messages, abortController }) => {
  const response = await fetch("/api/chat", {
    method: "POST",
    body: JSON.stringify({ threadId, messages }),
    headers: {
      "Content-Type": "application/json",
      Accept: "text/event-stream",
    },
    signal: abortController.signal,
  });
  return response;
};

export default function Home() {
  return <CrayonChat processMessage={processMessage} />;
}
```
  </TabItem>
</Tabs>

## Integrate LLM with your backend route

<Tabs groupId="backend-lang">
  <TabItem value="ts" label="TypeScript">
```typescript title="api/chat/route.ts"
import { NextRequest, NextResponse } from "next/server";
import OpenAI from "openai";
import {
  fromOpenAICompletion,
  templatesToResponseFormat,
  toOpenAIMessages,
} from "@crayonai/stream";
import { Message } from "@crayonai/react-core";

export async function POST(req: NextRequest) {
  const { messages } = (await req.json()) as { messages: Message[] };
  const client = new OpenAI();
  const llmStream = await client.chat.completions.create({
    model: "gpt-4o",
    messages: toOpenAIMessages(
      messages
    ) as OpenAI.Chat.ChatCompletionMessageParam[],
    stream: true,
    response_format: templatesToResponseFormat(),
  });
  const responseStream = fromOpenAICompletion(llmStream);
  return new NextResponse(
    responseStream as unknown as ReadableStream<Uint8Array>,
    {
      headers: {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache, no-transform",
        Connection: "keep-alive",
      },
    }
  );
}
```
  </TabItem>
  <TabItem value="js" label="JavaScript">
```javascript title="api/chat/route.js"
import { NextResponse } from "next/server";
import OpenAI from "openai";
import {
  fromOpenAICompletion,
  templatesToResponseFormat,
  toOpenAIMessages,
} from "@crayonai/stream";

export async function POST(req) {
  const { messages } = await req.json();
  const client = new OpenAI();
  const llmStream = await client.chat.completions.create({
    model: "gpt-4o",
    messages: toOpenAIMessages(messages),
    stream: true,
    response_format: templatesToResponseFormat(),
  });
  const responseStream = fromOpenAICompletion(llmStream);
  return new NextResponse(responseStream, {
    headers: {
      "Content-Type": "text/event-stream",
      "Cache-Control": "no-cache, no-transform",
      Connection: "keep-alive",
    },
  });
}
```
  </TabItem>
</Tabs>

### Serve the NextJS app

<Tabs groupId="js-package-manager">
  <TabItem value="npm" label="npm">
  ```bash
  npm run dev
    ```
  </TabItem>
  <TabItem value="pnpm" label="pnpm">
  ```bash
  pnpm dev
  ```
  </TabItem>
  <TabItem value="yarn" label="yarn">
  ```bash
  yarn dev
  ```
  </TabItem>
</Tabs>
